{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# extracting url\n",
    "import re\n",
    "# for KNN fill null\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset is downloaded from https://www.kaggle.com/datasets/andrewmvd/okcupid-profiles\n",
    "df = pd.read_csv(\"okcupid_profiles.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>status</th>\n",
       "      <th>sex</th>\n",
       "      <th>orientation</th>\n",
       "      <th>body_type</th>\n",
       "      <th>diet</th>\n",
       "      <th>drinks</th>\n",
       "      <th>drugs</th>\n",
       "      <th>education</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>...</th>\n",
       "      <th>essay0</th>\n",
       "      <th>essay1</th>\n",
       "      <th>essay2</th>\n",
       "      <th>essay3</th>\n",
       "      <th>essay4</th>\n",
       "      <th>essay5</th>\n",
       "      <th>essay6</th>\n",
       "      <th>essay7</th>\n",
       "      <th>essay8</th>\n",
       "      <th>essay9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>a little extra</td>\n",
       "      <td>strictly anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>asian, white</td>\n",
       "      <td>...</td>\n",
       "      <td>about me:  i would love to think that i was so...</td>\n",
       "      <td>currently working as an international agent fo...</td>\n",
       "      <td>making people laugh. ranting about a good salt...</td>\n",
       "      <td>the way i look. i am a six foot half asian, ha...</td>\n",
       "      <td>books: absurdistan, the republic, of mice and ...</td>\n",
       "      <td>food. water. cell phone. shelter.</td>\n",
       "      <td>duality and humorous things</td>\n",
       "      <td>trying to find someone to hang out with. i am ...</td>\n",
       "      <td>i am new to california and looking for someone...</td>\n",
       "      <td>you want to be swept off your feet! you are ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>average</td>\n",
       "      <td>mostly other</td>\n",
       "      <td>often</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>working on space camp</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>i am a chef: this is what that means. 1. i am ...</td>\n",
       "      <td>dedicating everyday to being an unbelievable b...</td>\n",
       "      <td>being silly. having ridiculous amonts of fun w...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i am die hard christopher moore fan. i don't r...</td>\n",
       "      <td>delicious porkness in all of its glories. my b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i am very open and will share just about anyth...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>available</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>thin</td>\n",
       "      <td>anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>NaN</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>i'm not ashamed of much, but writing public te...</td>\n",
       "      <td>i make nerdy software for musicians, artists, ...</td>\n",
       "      <td>improvising in different contexts. alternating...</td>\n",
       "      <td>my large jaw and large glasses are the physica...</td>\n",
       "      <td>okay this is where the cultural matrix gets so...</td>\n",
       "      <td>movement conversation creation contemplation t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>viewing. listening. dancing. talking. drinking...</td>\n",
       "      <td>when i was five years old, i was known as \"the...</td>\n",
       "      <td>you are bright, open, intense, silly, ironic, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>thin</td>\n",
       "      <td>vegetarian</td>\n",
       "      <td>socially</td>\n",
       "      <td>NaN</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>white</td>\n",
       "      <td>...</td>\n",
       "      <td>i work in a library and go to school. . .</td>\n",
       "      <td>reading things written by old dead people</td>\n",
       "      <td>playing synthesizers and organizing books acco...</td>\n",
       "      <td>socially awkward but i do my best</td>\n",
       "      <td>bataille, celine, beckett. . . lynch, jarmusch...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cats and german philosophy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>you feel so inclined.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>single</td>\n",
       "      <td>m</td>\n",
       "      <td>straight</td>\n",
       "      <td>athletic</td>\n",
       "      <td>NaN</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from college/university</td>\n",
       "      <td>asian, black, other</td>\n",
       "      <td>...</td>\n",
       "      <td>hey how's it going? currently vague on the pro...</td>\n",
       "      <td>work work work work + play</td>\n",
       "      <td>creating imagery to look at: http://bagsbrown....</td>\n",
       "      <td>i smile a lot and my inquisitive nature</td>\n",
       "      <td>music: bands, rappers, musicians at the moment...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age     status sex orientation       body_type               diet  \\\n",
       "0   22     single   m    straight  a little extra  strictly anything   \n",
       "1   35     single   m    straight         average       mostly other   \n",
       "2   38  available   m    straight            thin           anything   \n",
       "3   23     single   m    straight            thin         vegetarian   \n",
       "4   29     single   m    straight        athletic                NaN   \n",
       "\n",
       "     drinks      drugs                          education  \\\n",
       "0  socially      never      working on college/university   \n",
       "1     often  sometimes              working on space camp   \n",
       "2  socially        NaN     graduated from masters program   \n",
       "3  socially        NaN      working on college/university   \n",
       "4  socially      never  graduated from college/university   \n",
       "\n",
       "             ethnicity  ...  \\\n",
       "0         asian, white  ...   \n",
       "1                white  ...   \n",
       "2                  NaN  ...   \n",
       "3                white  ...   \n",
       "4  asian, black, other  ...   \n",
       "\n",
       "                                              essay0  \\\n",
       "0  about me:  i would love to think that i was so...   \n",
       "1  i am a chef: this is what that means. 1. i am ...   \n",
       "2  i'm not ashamed of much, but writing public te...   \n",
       "3          i work in a library and go to school. . .   \n",
       "4  hey how's it going? currently vague on the pro...   \n",
       "\n",
       "                                              essay1  \\\n",
       "0  currently working as an international agent fo...   \n",
       "1  dedicating everyday to being an unbelievable b...   \n",
       "2  i make nerdy software for musicians, artists, ...   \n",
       "3          reading things written by old dead people   \n",
       "4                         work work work work + play   \n",
       "\n",
       "                                              essay2  \\\n",
       "0  making people laugh. ranting about a good salt...   \n",
       "1  being silly. having ridiculous amonts of fun w...   \n",
       "2  improvising in different contexts. alternating...   \n",
       "3  playing synthesizers and organizing books acco...   \n",
       "4  creating imagery to look at: http://bagsbrown....   \n",
       "\n",
       "                                              essay3  \\\n",
       "0  the way i look. i am a six foot half asian, ha...   \n",
       "1                                                NaN   \n",
       "2  my large jaw and large glasses are the physica...   \n",
       "3                  socially awkward but i do my best   \n",
       "4            i smile a lot and my inquisitive nature   \n",
       "\n",
       "                                              essay4  \\\n",
       "0  books: absurdistan, the republic, of mice and ...   \n",
       "1  i am die hard christopher moore fan. i don't r...   \n",
       "2  okay this is where the cultural matrix gets so...   \n",
       "3  bataille, celine, beckett. . . lynch, jarmusch...   \n",
       "4  music: bands, rappers, musicians at the moment...   \n",
       "\n",
       "                                              essay5  \\\n",
       "0                  food. water. cell phone. shelter.   \n",
       "1  delicious porkness in all of its glories. my b...   \n",
       "2  movement conversation creation contemplation t...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                        essay6  \\\n",
       "0  duality and humorous things   \n",
       "1                          NaN   \n",
       "2                          NaN   \n",
       "3   cats and german philosophy   \n",
       "4                          NaN   \n",
       "\n",
       "                                              essay7  \\\n",
       "0  trying to find someone to hang out with. i am ...   \n",
       "1                                                NaN   \n",
       "2  viewing. listening. dancing. talking. drinking...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              essay8  \\\n",
       "0  i am new to california and looking for someone...   \n",
       "1  i am very open and will share just about anyth...   \n",
       "2  when i was five years old, i was known as \"the...   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "\n",
       "                                              essay9  \n",
       "0  you want to be swept off your feet! you are ti...  \n",
       "1                                                NaN  \n",
       "2  you are bright, open, intense, silly, ironic, ...  \n",
       "3                              you feel so inclined.  \n",
       "4                                                NaN  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check top 5 rows of dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59946 entries, 0 to 59945\n",
      "Data columns (total 31 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   age          59946 non-null  int64  \n",
      " 1   status       59946 non-null  object \n",
      " 2   sex          59946 non-null  object \n",
      " 3   orientation  59946 non-null  object \n",
      " 4   body_type    54650 non-null  object \n",
      " 5   diet         35551 non-null  object \n",
      " 6   drinks       56961 non-null  object \n",
      " 7   drugs        45866 non-null  object \n",
      " 8   education    53318 non-null  object \n",
      " 9   ethnicity    54266 non-null  object \n",
      " 10  height       59943 non-null  float64\n",
      " 11  income       59946 non-null  int64  \n",
      " 12  job          51748 non-null  object \n",
      " 13  last_online  59946 non-null  object \n",
      " 14  location     59946 non-null  object \n",
      " 15  offspring    24385 non-null  object \n",
      " 16  pets         40025 non-null  object \n",
      " 17  religion     39720 non-null  object \n",
      " 18  sign         48890 non-null  object \n",
      " 19  smokes       54434 non-null  object \n",
      " 20  speaks       59896 non-null  object \n",
      " 21  essay0       54458 non-null  object \n",
      " 22  essay1       52374 non-null  object \n",
      " 23  essay2       50308 non-null  object \n",
      " 24  essay3       48470 non-null  object \n",
      " 25  essay4       49409 non-null  object \n",
      " 26  essay5       49096 non-null  object \n",
      " 27  essay6       46175 non-null  object \n",
      " 28  essay7       47495 non-null  object \n",
      " 29  essay8       40721 non-null  object \n",
      " 30  essay9       47343 non-null  object \n",
      "dtypes: float64(1), int64(2), object(28)\n",
      "memory usage: 14.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# check column name, data type\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>height</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>59946.000000</td>\n",
       "      <td>59943.000000</td>\n",
       "      <td>59946.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>32.340290</td>\n",
       "      <td>68.295281</td>\n",
       "      <td>20033.222534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.452779</td>\n",
       "      <td>3.994803</td>\n",
       "      <td>97346.192104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>30.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>37.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>110.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>1000000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                age        height          income\n",
       "count  59946.000000  59943.000000    59946.000000\n",
       "mean      32.340290     68.295281    20033.222534\n",
       "std        9.452779      3.994803    97346.192104\n",
       "min       18.000000      1.000000       -1.000000\n",
       "25%       26.000000     66.000000       -1.000000\n",
       "50%       30.000000     68.000000       -1.000000\n",
       "75%       37.000000     71.000000       -1.000000\n",
       "max      110.000000     95.000000  1000000.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check numerical data \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                0\n",
       "status             0\n",
       "sex                0\n",
       "orientation        0\n",
       "body_type       5296\n",
       "diet           24395\n",
       "drinks          2985\n",
       "drugs          14080\n",
       "education       6628\n",
       "ethnicity       5680\n",
       "height             3\n",
       "income             0\n",
       "job             8198\n",
       "last_online        0\n",
       "location           0\n",
       "offspring      35561\n",
       "pets           19921\n",
       "religion       20226\n",
       "sign           11056\n",
       "smokes          5512\n",
       "speaks            50\n",
       "essay0          5488\n",
       "essay1          7572\n",
       "essay2          9638\n",
       "essay3         11476\n",
       "essay4         10537\n",
       "essay5         10850\n",
       "essay6         13771\n",
       "essay7         12451\n",
       "essay8         19225\n",
       "essay9         12603\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check null value count\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleanse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sex_height(df):\n",
    "    \"\"\"\"\n",
    "    This function will convert\n",
    "    - 'm' to male and 'f' to female\n",
    "    - height from inches to cm\n",
    "    \"\"\"\n",
    "    # make a copy of the dataframe\n",
    "    df = df.copy()\n",
    "    # convert 'm' to 'male' and 'f' to 'female'\n",
    "    df['sex'] = df['sex'].map({'m': 'male', 'f': 'female'})\n",
    "    # convert height from inches to cm\n",
    "    df['height'] = round(df['height'] * 2.54)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Null by Meaningful text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df):\n",
    "    \"\"\"\n",
    "    This function will fill missing values with meaningful text, combine essays into one column and drop columns that are not needed\n",
    "\n",
    "    \"\"\"\n",
    "    # make a copy of the dataframe\n",
    "    df = df.copy()\n",
    "    # fill missing values with meaningful text\n",
    "    df['diet'] = df['diet'].fillna(\"anything\")\n",
    "    df['drinks'] = df['drinks'].fillna(\"not at all\")\n",
    "    df['drugs'] = df['drugs'].fillna(\"never\")\n",
    "    df['height'] = df['height'].fillna(df['height'].median())\n",
    "    df['job'] = df['job'].fillna('other')\n",
    "    df['offspring'] = df['offspring'].fillna(\"no kids and neutral to kids\")\n",
    "    df['pets'] = df['pets'].fillna(\"no pets and neutral to pets\")\n",
    "    df['religion'] = df['religion'].fillna(\"irreligion\")\n",
    "    df['sign'] = df['sign'].str.replace(\"&rsquo;\", \"'\").fillna(\"unknown zodiac sign\")\n",
    "    df['smokes'] = df['smokes'].fillna(\"no\")\n",
    "    df['speaks'] = df['speaks'].fillna(\"english\")\n",
    "    # filling missing values for essays, combine essays into one column and remove excess \",\"\n",
    "    df['essay_all'] = df.loc[:,\"essay0\":\"essay9\"].fillna(\"\").apply(lambda x: ','.join(x.astype(str)),axis=1).str.strip(\",\").fillna(\"\")\n",
    "    # drop columns that are not needed\n",
    "    columns_to_drop = ['essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6','essay7', 'essay8', 'essay9', 'last_online']\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fill Value by KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values_knn(df, col_fill, list_col_ref, list_col_num):\n",
    "    \"\"\"\n",
    "    This function will fill missing values using KNN imputer\n",
    "    \"\"\"\n",
    "\n",
    "    # Copy dataframe to avoid modifying original dataframe\n",
    "    df = df.copy()\n",
    "\n",
    "    # Create a label encoder object\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # drop rows with missing values in the column to be imputed\n",
    "    df_non_null = df[col_fill].dropna()\n",
    "    # Fit and transform the column to be imputed\n",
    "    df.loc[df_non_null.index, col_fill +'_encoded'] = label_encoder.fit_transform(df_non_null)\n",
    "\n",
    "    # Convert the column to float\n",
    "    df[col_fill + '_encoded'] = df[col_fill + '_encoded'].astype(float)\n",
    "    # Fill missing values with NaN\n",
    "    df.loc[df[col_fill].isnull(), col_fill + '_encoded'] = np.nan\n",
    "\n",
    "    # Print the mapping of the label encoder\n",
    "    print(\"Label Encoding Mapping: \", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n",
    "\n",
    "    # Convert categorical variables into numerical format\n",
    "    for col in list_col_ref:\n",
    "        df[col + '_encoded'] = LabelEncoder().fit_transform(df[col].astype(str))  # Encode as numbers\n",
    "\n",
    "    # List of features to be used for KNN including numerical columns, encoded categorical columns and encoded column to be imputed\n",
    "    knn_features = list_col_num + [col + '_encoded' for col in list_col_ref] + [col_fill + '_encoded']\n",
    "\n",
    "    # Ensure only features to be used for KNN are selected\n",
    "    df_knn = df[knn_features]\n",
    "\n",
    "    # Initialize KNN Imputer with 5 neighbors\n",
    "    imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    # Fit and transform the KNN imputer\n",
    "    df_knn_imputed = imputer.fit_transform(df_knn)\n",
    "\n",
    "    # Replace original dataframe values with imputed values\n",
    "    df[knn_features] = df_knn_imputed\n",
    "\n",
    "    # Round to nearest integer before inverse transforming\n",
    "    df[col_fill + '_encoded'] = df[col_fill + '_encoded'].round().astype(int)\n",
    "\n",
    "    # Inverse transform the encoded column to get the original values\n",
    "    df[col_fill + '_imputed'] = label_encoder.inverse_transform(df[col_fill + '_encoded'])\n",
    "\n",
    "    # Replace the original column with the imputed values\n",
    "    df[col_fill] = df[col_fill + '_imputed']\n",
    "\n",
    "    # Drop columns that are no longer needed\n",
    "    df = df.drop(columns=[col_fill + '_encoded', col_fill + '_imputed'] + [col + '_encoded' for col in list_col_ref])\n",
    "\n",
    "    return df\n",
    "\n",
    "def fill_missing_values_knn_multi_col(df, col_fillna):\n",
    "    \"\"\"\n",
    "    This function will fill missing values using KNN imputer for multiple columns\n",
    "    \"\"\"\n",
    "    # Copy dataframe to avoid modifying original dataframe\n",
    "    df = df.copy()\n",
    "    # fill missing values for each key in the dictionary\n",
    "    for col_fill, list_col_ref in col_fillna.items():\n",
    "        df = fill_missing_values_knn(df, col_fill, list_col_ref[0], list_col_ref[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract and Remove URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_urls(text):\n",
    "    \"\"\"\n",
    "    This function will extract URLs from text for one row\n",
    "    \"\"\"\n",
    "    # Check if text is a string\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    # extract URLs\n",
    "    url_pattern = r'\\b(?=https?:\\/\\/|www\\.[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6})\\b([-a-zA-Z0-9(@:%_\\+.~#?&//=]*)'\n",
    "    # Extract matches\n",
    "    matches = re.findall(url_pattern, text)\n",
    "    return matches\n",
    "\n",
    "def remove_url(row):\n",
    "    \"\"\"\"\n",
    "    This function will remove URL from text for one row\n",
    "    \"\"\"\n",
    "    # Get essay text\n",
    "    essay = row['essay_all']\n",
    "    # Remove URL from text\n",
    "    for url in row['url']:\n",
    "        essay = essay.replace(url, '')\n",
    "    return essay\n",
    "\n",
    "def remove_url_col(df):\n",
    "    \"\"\"\n",
    "    This function will remove URL from text for each row in the dataframe\n",
    "    \"\"\"\n",
    "    # Copy dataframe to avoid modifying original dataframe\n",
    "    df = df.copy()\n",
    "    # Extract URLs for each row\n",
    "    df['url'] = df['essay_all'].apply(extract_urls)\n",
    "    # Remove URLs for each row\n",
    "    df['essay_all'] = df.apply(remove_url, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Cleanse Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(df, col_fillna):\n",
    "    \"\"\"\n",
    "    This function will clean the dataset by:\n",
    "    - Converting\n",
    "        - 'm' to 'male'\n",
    "        - 'f' to 'female'\n",
    "        - height from inches to cm \n",
    "    - Filling missing values with meaningful text\n",
    "    - Filling missing values using KNN imputer\n",
    "    - Removing URL from text\n",
    "    \"\"\"\n",
    "    # Copy dataframe to avoid modifying original dataframe\n",
    "    df = df.copy()\n",
    "    df = process_sex_height(df)\n",
    "    df = fill_missing_values(df)\n",
    "    df = fill_missing_values_knn_multi_col(df, col_fillna)\n",
    "    df = remove_url_col(df)\n",
    "    # Drop columns that are not needed\n",
    "    df = df.drop(columns=['income','url'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of columns to fillna as key, list of columns to be used as reference and numerical columns as value\n",
    "col_fillna = {'body_type': [['sex', 'drinks', 'diet', 'drugs'], ['age', 'height']],\n",
    "              'education': [['job', 'location'], ['age', 'income']],\n",
    "              'ethnicity': [['religion', 'location'], ['age']]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Encoding Mapping:  {'a little extra': 0, 'athletic': 1, 'average': 2, 'curvy': 3, 'fit': 4, 'full figured': 5, 'jacked': 6, 'overweight': 7, 'rather not say': 8, 'skinny': 9, 'thin': 10, 'used up': 11}\n",
      "Label Encoding Mapping:  {'college/university': 0, 'dropped out of college/university': 1, 'dropped out of high school': 2, 'dropped out of law school': 3, 'dropped out of masters program': 4, 'dropped out of med school': 5, 'dropped out of ph.d program': 6, 'dropped out of space camp': 7, 'dropped out of two-year college': 8, 'graduated from college/university': 9, 'graduated from high school': 10, 'graduated from law school': 11, 'graduated from masters program': 12, 'graduated from med school': 13, 'graduated from ph.d program': 14, 'graduated from space camp': 15, 'graduated from two-year college': 16, 'high school': 17, 'law school': 18, 'masters program': 19, 'med school': 20, 'ph.d program': 21, 'space camp': 22, 'two-year college': 23, 'working on college/university': 24, 'working on high school': 25, 'working on law school': 26, 'working on masters program': 27, 'working on med school': 28, 'working on ph.d program': 29, 'working on space camp': 30, 'working on two-year college': 31}\n",
      "Label Encoding Mapping:  {'asian': 0, 'asian, black': 1, 'asian, black, hispanic / latin': 2, 'asian, black, hispanic / latin, other': 3, 'asian, black, hispanic / latin, white': 4, 'asian, black, hispanic / latin, white, other': 5, 'asian, black, indian': 6, 'asian, black, indian, hispanic / latin, other': 7, 'asian, black, native american': 8, 'asian, black, native american, hispanic / latin': 9, 'asian, black, native american, hispanic / latin, white': 10, 'asian, black, native american, indian': 11, 'asian, black, native american, indian, hispanic / latin, white, other': 12, 'asian, black, native american, indian, pacific islander, hispanic / latin': 13, 'asian, black, native american, indian, pacific islander, white': 14, 'asian, black, native american, other': 15, 'asian, black, native american, pacific islander': 16, 'asian, black, native american, pacific islander, other': 17, 'asian, black, native american, pacific islander, white': 18, 'asian, black, native american, pacific islander, white, other': 19, 'asian, black, native american, white': 20, 'asian, black, native american, white, other': 21, 'asian, black, other': 22, 'asian, black, pacific islander': 23, 'asian, black, pacific islander, hispanic / latin': 24, 'asian, black, pacific islander, hispanic / latin, white': 25, 'asian, black, pacific islander, other': 26, 'asian, black, pacific islander, white': 27, 'asian, black, pacific islander, white, other': 28, 'asian, black, white': 29, 'asian, black, white, other': 30, 'asian, hispanic / latin': 31, 'asian, hispanic / latin, other': 32, 'asian, hispanic / latin, white': 33, 'asian, hispanic / latin, white, other': 34, 'asian, indian': 35, 'asian, indian, hispanic / latin': 36, 'asian, indian, hispanic / latin, other': 37, 'asian, indian, hispanic / latin, white': 38, 'asian, indian, other': 39, 'asian, indian, pacific islander': 40, 'asian, indian, pacific islander, hispanic / latin, white, other': 41, 'asian, indian, pacific islander, other': 42, 'asian, indian, white': 43, 'asian, indian, white, other': 44, 'asian, middle eastern': 45, 'asian, middle eastern, black': 46, 'asian, middle eastern, black, indian, pacific islander, hispanic / latin, white': 47, 'asian, middle eastern, black, native american, hispanic / latin, white': 48, 'asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin': 49, 'asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin, other': 50, 'asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin, white': 51, 'asian, middle eastern, black, native american, indian, pacific islander, hispanic / latin, white, other': 52, 'asian, middle eastern, black, native american, indian, pacific islander, white': 53, 'asian, middle eastern, black, native american, pacific islander, hispanic / latin, white, other': 54, 'asian, middle eastern, black, pacific islander': 55, 'asian, middle eastern, black, pacific islander, hispanic / latin': 56, 'asian, middle eastern, black, pacific islander, hispanic / latin, white': 57, 'asian, middle eastern, black, white, other': 58, 'asian, middle eastern, hispanic / latin': 59, 'asian, middle eastern, hispanic / latin, white': 60, 'asian, middle eastern, hispanic / latin, white, other': 61, 'asian, middle eastern, indian': 62, 'asian, middle eastern, indian, hispanic / latin': 63, 'asian, middle eastern, indian, hispanic / latin, white, other': 64, 'asian, middle eastern, indian, other': 65, 'asian, middle eastern, native american, hispanic / latin, white': 66, 'asian, middle eastern, native american, indian, pacific islander, hispanic / latin, white': 67, 'asian, middle eastern, native american, pacific islander, hispanic / latin, white, other': 68, 'asian, middle eastern, native american, pacific islander, other': 69, 'asian, middle eastern, native american, pacific islander, white, other': 70, 'asian, middle eastern, other': 71, 'asian, middle eastern, white': 72, 'asian, middle eastern, white, other': 73, 'asian, native american': 74, 'asian, native american, hispanic / latin': 75, 'asian, native american, hispanic / latin, other': 76, 'asian, native american, hispanic / latin, white': 77, 'asian, native american, hispanic / latin, white, other': 78, 'asian, native american, indian, pacific islander, hispanic / latin, white': 79, 'asian, native american, indian, pacific islander, hispanic / latin, white, other': 80, 'asian, native american, other': 81, 'asian, native american, pacific islander': 82, 'asian, native american, pacific islander, hispanic / latin, white': 83, 'asian, native american, pacific islander, hispanic / latin, white, other': 84, 'asian, native american, pacific islander, white': 85, 'asian, native american, pacific islander, white, other': 86, 'asian, native american, white': 87, 'asian, native american, white, other': 88, 'asian, other': 89, 'asian, pacific islander': 90, 'asian, pacific islander, hispanic / latin': 91, 'asian, pacific islander, hispanic / latin, other': 92, 'asian, pacific islander, hispanic / latin, white': 93, 'asian, pacific islander, hispanic / latin, white, other': 94, 'asian, pacific islander, other': 95, 'asian, pacific islander, white': 96, 'asian, pacific islander, white, other': 97, 'asian, white': 98, 'asian, white, other': 99, 'black': 100, 'black, hispanic / latin': 101, 'black, hispanic / latin, other': 102, 'black, hispanic / latin, white': 103, 'black, hispanic / latin, white, other': 104, 'black, indian': 105, 'black, indian, hispanic / latin': 106, 'black, indian, hispanic / latin, white': 107, 'black, indian, other': 108, 'black, indian, white': 109, 'black, indian, white, other': 110, 'black, native american': 111, 'black, native american, hispanic / latin': 112, 'black, native american, hispanic / latin, other': 113, 'black, native american, hispanic / latin, white': 114, 'black, native american, hispanic / latin, white, other': 115, 'black, native american, indian': 116, 'black, native american, indian, hispanic / latin, white, other': 117, 'black, native american, indian, other': 118, 'black, native american, indian, pacific islander': 119, 'black, native american, indian, pacific islander, hispanic / latin': 120, 'black, native american, indian, white': 121, 'black, native american, indian, white, other': 122, 'black, native american, other': 123, 'black, native american, pacific islander': 124, 'black, native american, pacific islander, hispanic / latin': 125, 'black, native american, pacific islander, hispanic / latin, white': 126, 'black, native american, pacific islander, hispanic / latin, white, other': 127, 'black, native american, pacific islander, other': 128, 'black, native american, pacific islander, white': 129, 'black, native american, pacific islander, white, other': 130, 'black, native american, white': 131, 'black, native american, white, other': 132, 'black, other': 133, 'black, pacific islander': 134, 'black, pacific islander, hispanic / latin': 135, 'black, pacific islander, other': 136, 'black, pacific islander, white': 137, 'black, white': 138, 'black, white, other': 139, 'hispanic / latin': 140, 'hispanic / latin, other': 141, 'hispanic / latin, white': 142, 'hispanic / latin, white, other': 143, 'indian': 144, 'indian, hispanic / latin': 145, 'indian, hispanic / latin, other': 146, 'indian, hispanic / latin, white': 147, 'indian, hispanic / latin, white, other': 148, 'indian, other': 149, 'indian, pacific islander': 150, 'indian, pacific islander, hispanic / latin, white': 151, 'indian, white': 152, 'indian, white, other': 153, 'middle eastern': 154, 'middle eastern, black': 155, 'middle eastern, black, hispanic / latin': 156, 'middle eastern, black, indian, pacific islander, hispanic / latin, white': 157, 'middle eastern, black, native american, hispanic / latin, white': 158, 'middle eastern, black, native american, indian': 159, 'middle eastern, black, native american, indian, hispanic / latin, white': 160, 'middle eastern, black, native american, indian, pacific islander, hispanic / latin, white': 161, 'middle eastern, black, native american, indian, pacific islander, hispanic / latin, white, other': 162, 'middle eastern, black, native american, indian, white, other': 163, 'middle eastern, black, native american, white': 164, 'middle eastern, black, native american, white, other': 165, 'middle eastern, black, other': 166, 'middle eastern, black, pacific islander, white': 167, 'middle eastern, black, white': 168, 'middle eastern, hispanic / latin': 169, 'middle eastern, hispanic / latin, other': 170, 'middle eastern, hispanic / latin, white': 171, 'middle eastern, hispanic / latin, white, other': 172, 'middle eastern, indian': 173, 'middle eastern, indian, other': 174, 'middle eastern, indian, white': 175, 'middle eastern, indian, white, other': 176, 'middle eastern, native american': 177, 'middle eastern, native american, hispanic / latin': 178, 'middle eastern, native american, hispanic / latin, white': 179, 'middle eastern, native american, hispanic / latin, white, other': 180, 'middle eastern, native american, white': 181, 'middle eastern, native american, white, other': 182, 'middle eastern, other': 183, 'middle eastern, pacific islander': 184, 'middle eastern, pacific islander, hispanic / latin': 185, 'middle eastern, pacific islander, other': 186, 'middle eastern, white': 187, 'middle eastern, white, other': 188, 'native american': 189, 'native american, hispanic / latin': 190, 'native american, hispanic / latin, other': 191, 'native american, hispanic / latin, white': 192, 'native american, hispanic / latin, white, other': 193, 'native american, indian': 194, 'native american, indian, pacific islander, hispanic / latin': 195, 'native american, indian, white': 196, 'native american, other': 197, 'native american, pacific islander': 198, 'native american, pacific islander, hispanic / latin': 199, 'native american, pacific islander, hispanic / latin, white': 200, 'native american, pacific islander, hispanic / latin, white, other': 201, 'native american, pacific islander, white': 202, 'native american, pacific islander, white, other': 203, 'native american, white': 204, 'native american, white, other': 205, 'other': 206, 'pacific islander': 207, 'pacific islander, hispanic / latin': 208, 'pacific islander, hispanic / latin, other': 209, 'pacific islander, hispanic / latin, white': 210, 'pacific islander, hispanic / latin, white, other': 211, 'pacific islander, other': 212, 'pacific islander, white': 213, 'pacific islander, white, other': 214, 'white': 215, 'white, other': 216}\n"
     ]
    }
   ],
   "source": [
    "# Call the data_cleaning function\n",
    "df_cleaned = data_cleaning(df, col_fillna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>status</th>\n",
       "      <th>sex</th>\n",
       "      <th>orientation</th>\n",
       "      <th>body_type</th>\n",
       "      <th>diet</th>\n",
       "      <th>drinks</th>\n",
       "      <th>drugs</th>\n",
       "      <th>education</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>height</th>\n",
       "      <th>job</th>\n",
       "      <th>location</th>\n",
       "      <th>offspring</th>\n",
       "      <th>pets</th>\n",
       "      <th>religion</th>\n",
       "      <th>sign</th>\n",
       "      <th>smokes</th>\n",
       "      <th>speaks</th>\n",
       "      <th>essay_all</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.0</td>\n",
       "      <td>single</td>\n",
       "      <td>male</td>\n",
       "      <td>straight</td>\n",
       "      <td>a little extra</td>\n",
       "      <td>strictly anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>asian, white</td>\n",
       "      <td>190.0</td>\n",
       "      <td>transportation</td>\n",
       "      <td>south san francisco, california</td>\n",
       "      <td>doesn't have kids, but might want them</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>agnosticism and very serious about it</td>\n",
       "      <td>gemini</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>english</td>\n",
       "      <td>about me:  i would love to think that i was so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35.0</td>\n",
       "      <td>single</td>\n",
       "      <td>male</td>\n",
       "      <td>straight</td>\n",
       "      <td>average</td>\n",
       "      <td>mostly other</td>\n",
       "      <td>often</td>\n",
       "      <td>sometimes</td>\n",
       "      <td>working on space camp</td>\n",
       "      <td>white</td>\n",
       "      <td>178.0</td>\n",
       "      <td>hospitality / travel</td>\n",
       "      <td>oakland, california</td>\n",
       "      <td>doesn't have kids, but might want them</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>agnosticism but not too serious about it</td>\n",
       "      <td>cancer</td>\n",
       "      <td>no</td>\n",
       "      <td>english (fluently), spanish (poorly), french (...</td>\n",
       "      <td>i am a chef: this is what that means. 1. i am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38.0</td>\n",
       "      <td>available</td>\n",
       "      <td>male</td>\n",
       "      <td>straight</td>\n",
       "      <td>thin</td>\n",
       "      <td>anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from masters program</td>\n",
       "      <td>black, indian, hispanic / latin</td>\n",
       "      <td>173.0</td>\n",
       "      <td>other</td>\n",
       "      <td>san francisco, california</td>\n",
       "      <td>no kids and neutral to kids</td>\n",
       "      <td>has cats</td>\n",
       "      <td>irreligion</td>\n",
       "      <td>pisces but it doesn't matter</td>\n",
       "      <td>no</td>\n",
       "      <td>english, french, c++</td>\n",
       "      <td>i'm not ashamed of much, but writing public te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23.0</td>\n",
       "      <td>single</td>\n",
       "      <td>male</td>\n",
       "      <td>straight</td>\n",
       "      <td>thin</td>\n",
       "      <td>vegetarian</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>working on college/university</td>\n",
       "      <td>white</td>\n",
       "      <td>180.0</td>\n",
       "      <td>student</td>\n",
       "      <td>berkeley, california</td>\n",
       "      <td>doesn't want kids</td>\n",
       "      <td>likes cats</td>\n",
       "      <td>irreligion</td>\n",
       "      <td>pisces</td>\n",
       "      <td>no</td>\n",
       "      <td>english, german (poorly)</td>\n",
       "      <td>i work in a library and go to school. . .,read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.0</td>\n",
       "      <td>single</td>\n",
       "      <td>male</td>\n",
       "      <td>straight</td>\n",
       "      <td>athletic</td>\n",
       "      <td>anything</td>\n",
       "      <td>socially</td>\n",
       "      <td>never</td>\n",
       "      <td>graduated from college/university</td>\n",
       "      <td>asian, black, other</td>\n",
       "      <td>168.0</td>\n",
       "      <td>artistic / musical / writer</td>\n",
       "      <td>san francisco, california</td>\n",
       "      <td>no kids and neutral to kids</td>\n",
       "      <td>likes dogs and likes cats</td>\n",
       "      <td>irreligion</td>\n",
       "      <td>aquarius</td>\n",
       "      <td>no</td>\n",
       "      <td>english</td>\n",
       "      <td>hey how's it going? currently vague on the pro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age     status   sex orientation       body_type               diet  \\\n",
       "0  22.0     single  male    straight  a little extra  strictly anything   \n",
       "1  35.0     single  male    straight         average       mostly other   \n",
       "2  38.0  available  male    straight            thin           anything   \n",
       "3  23.0     single  male    straight            thin         vegetarian   \n",
       "4  29.0     single  male    straight        athletic           anything   \n",
       "\n",
       "     drinks      drugs                          education  \\\n",
       "0  socially      never      working on college/university   \n",
       "1     often  sometimes              working on space camp   \n",
       "2  socially      never     graduated from masters program   \n",
       "3  socially      never      working on college/university   \n",
       "4  socially      never  graduated from college/university   \n",
       "\n",
       "                         ethnicity  height                          job  \\\n",
       "0                     asian, white   190.0               transportation   \n",
       "1                            white   178.0         hospitality / travel   \n",
       "2  black, indian, hispanic / latin   173.0                        other   \n",
       "3                            white   180.0                      student   \n",
       "4              asian, black, other   168.0  artistic / musical / writer   \n",
       "\n",
       "                          location                               offspring  \\\n",
       "0  south san francisco, california  doesn't have kids, but might want them   \n",
       "1              oakland, california  doesn't have kids, but might want them   \n",
       "2        san francisco, california             no kids and neutral to kids   \n",
       "3             berkeley, california                       doesn't want kids   \n",
       "4        san francisco, california             no kids and neutral to kids   \n",
       "\n",
       "                        pets                                  religion  \\\n",
       "0  likes dogs and likes cats     agnosticism and very serious about it   \n",
       "1  likes dogs and likes cats  agnosticism but not too serious about it   \n",
       "2                   has cats                                irreligion   \n",
       "3                 likes cats                                irreligion   \n",
       "4  likes dogs and likes cats                                irreligion   \n",
       "\n",
       "                           sign     smokes  \\\n",
       "0                        gemini  sometimes   \n",
       "1                        cancer         no   \n",
       "2  pisces but it doesn't matter         no   \n",
       "3                        pisces         no   \n",
       "4                      aquarius         no   \n",
       "\n",
       "                                              speaks  \\\n",
       "0                                            english   \n",
       "1  english (fluently), spanish (poorly), french (...   \n",
       "2                               english, french, c++   \n",
       "3                           english, german (poorly)   \n",
       "4                                            english   \n",
       "\n",
       "                                           essay_all  \n",
       "0  about me:  i would love to think that i was so...  \n",
       "1  i am a chef: this is what that means. 1. i am ...  \n",
       "2  i'm not ashamed of much, but writing public te...  \n",
       "3  i work in a library and go to school. . .,read...  \n",
       "4  hey how's it going? currently vague on the pro...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check top 5 rows of cleaned dataset\n",
    "df_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Cleansed Dataset as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save cleaned dataset as csv\n",
    "df_cleaned.to_csv(\"okcupid_profiles_cleaned.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if the cleaned dataset is saved correctly\n",
    "pd.read_csv(\"okcupid_profiles_cleaned.csv\").fillna(\"\").equals(df_cleaned)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
